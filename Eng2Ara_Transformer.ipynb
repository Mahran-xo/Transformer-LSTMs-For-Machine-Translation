{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEgLcIkMBpz3",
        "outputId": "3138c4db-f27b-42fb-c788-cf1d753b0b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFbsAZH8A8tS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f2b951-99b0-443e-f923-d94c74240bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import callbacks\n",
        "from nltk import word_tokenize\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5vB1Q8sA8tT"
      },
      "outputs": [],
      "source": [
        "text_file = pathlib.Path('/content/drive/MyDrive/NLP Project/Dataset/Txt/ara_eng.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99DPbssxA8tU"
      },
      "outputs": [],
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, ar = line.split(\"\\t\")\n",
        "    ar = \"[start] \" + ar + \" [end]\"\n",
        "    text_pairs.append((eng, ar))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1EQoLj4A8tU"
      },
      "source": [
        "Here's what our sentence pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NBz9ez_A8tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582eb06d-fca4-4cba-bf16-5df473d6a9c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Tom and Mary have a small farm.', '[start] لتوم وماري مزرعة صغيرة. [end]')\n",
            "('but before we go into bolivar s time in peru let s get to know his personality journalist alvaro vargas llosa writes in an article that bolivar was a better warlord than all the other latin american leaders from that time but that warlordism itself is still the heart of the latin problem.', '[start] قبل ان نعرف ما فعله بوليفار اثناء بقايه في البيرو دعونا نتعرف على شخصيته يكتب الصحفي الفارو فاراكاس يوسا في احدى مقالاته ان بوليفار كان سيد حرب وافضل من جميع قادة امريكا اللاتينية في ذلك الزمن لكن كونه سيد حرب كان قلب المشكة اللاتينية [end]')\n",
            "('humans of costa rica a page created in july of has more than likes.', '[start] اناس من كوستاريكا صفحة تم انشاوها في شهر يوليو تموز من عام لديها اكثر من معجب [end]')\n",
            "('This play has three acts.', '[start] لهذه المسرحية ثلاثة فصول. [end]')\n",
            "('on july the hackers were hacked resulting in the release of a gb trove of documents demonstrating among other things that hacking team sold its software to repressive governments something the company had previously denied.', '[start] في الخامس من يوليو تموز تعرض المخترقون للاختراق مما نتج عن تسريب اربعماية جيجا بايت من المستندات التي توكد ضمن اشياء اخرى بيع شركة هاكينج تيم برمجياتها لحكومات قمعية وهو شيء نفته الشركة قبل ذلك [end]')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSlr4dK4A8tU"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdqHwCMsA8tV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32524bb-c46c-4926-b1eb-24048689f903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24638 total pairs\n",
            "17248 training pairs\n",
            "3695 validation pairs\n",
            "3695 test pairs\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjtpPpaeA8tV"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation +\"؟\"+\"[٠١٢٣٤٥٦٧٨٩]\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 20000\n",
        "sequence_length = 150\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "ar_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_ar_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "ar_vectorization.adapt(train_ar_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHrJSQHGA8tW"
      },
      "outputs": [],
      "source": [
        "def format_dataset(eng, ar):\n",
        "    eng = eng_vectorization(eng)\n",
        "    ar = ar_vectorization(ar)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ar[:, :-1],}, ar[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ar_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    ar_texts = list(ar_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ar_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbfH6UsGA8tW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc46f2d-a84e-4838-e28e-823fb672a68c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (128, 150)\n",
            "inputs[\"decoder_inputs\"].shape: (128, 150)\n",
            "targets.shape: (128, 150)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L7ad hena"
      ],
      "metadata": {
        "id": "-GPVmslZrPFa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDJ7MPn6A8tW"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2JWdXdvA8tY"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "id": "y7o5cjeTETgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15659642-1422-41a2-d1e9-c8cc29779846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding (Position  (None, None, 256)   5158400     ['encoder_inputs[0][0]']         \n",
            " alEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
            " erEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, None, 20000)  15557920    ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,871,776\n",
            "Trainable params: 23,871,776\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "early_stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=True)\n",
        "checkpoint = callbacks.ModelCheckpoint('/content/drive/MyDrive/NLP Project/FinalTransformer.h5', monitor='val_accuracy', verbose=True, save_best_only=True)"
      ],
      "metadata": {
        "id": "c4W5EEBNH3xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IybWE3hRA8tY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2b4390-0e82-4ed7-efd6-4602cdef489b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "135/135 [==============================] - ETA: 0s - loss: 7.1957 - accuracy: 0.1711\n",
            "Epoch 1: val_accuracy improved from -inf to 0.23924, saving model to /content/drive/MyDrive/NLP Project/FinalTransformer.h5\n",
            "135/135 [==============================] - 160s 1s/step - loss: 7.1957 - accuracy: 0.1711 - val_loss: 6.1960 - val_accuracy: 0.2392\n",
            "Epoch 2/40\n",
            "135/135 [==============================] - ETA: 0s - loss: 6.3847 - accuracy: 0.2121\n",
            "Epoch 2: val_accuracy improved from 0.23924 to 0.25785, saving model to /content/drive/MyDrive/NLP Project/FinalTransformer.h5\n",
            "135/135 [==============================] - 119s 879ms/step - loss: 6.3847 - accuracy: 0.2121 - val_loss: 5.8491 - val_accuracy: 0.2578\n",
            "Epoch 3/40\n",
            "135/135 [==============================] - ETA: 0s - loss: 5.8270 - accuracy: 0.2403\n",
            "Epoch 3: val_accuracy improved from 0.25785 to 0.26933, saving model to /content/drive/MyDrive/NLP Project/FinalTransformer.h5\n",
            "135/135 [==============================] - 119s 879ms/step - loss: 5.8270 - accuracy: 0.2403 - val_loss: 5.6682 - val_accuracy: 0.2693\n",
            "Epoch 4/40\n",
            "135/135 [==============================] - ETA: 0s - loss: 5.3148 - accuracy: 0.2661\n",
            "Epoch 4: val_accuracy improved from 0.26933 to 0.27448, saving model to /content/drive/MyDrive/NLP Project/FinalTransformer.h5\n",
            "135/135 [==============================] - 128s 950ms/step - loss: 5.3148 - accuracy: 0.2661 - val_loss: 5.5858 - val_accuracy: 0.2745\n",
            "Epoch 5/40\n",
            "135/135 [==============================] - ETA: 0s - loss: 4.8595 - accuracy: 0.2897\n",
            "Epoch 5: val_accuracy did not improve from 0.27448\n",
            "135/135 [==============================] - 116s 861ms/step - loss: 4.8595 - accuracy: 0.2897 - val_loss: 5.5711 - val_accuracy: 0.2639\n",
            "Epoch 6/40\n",
            "135/135 [==============================] - ETA: 0s - loss: 4.4547 - accuracy: 0.3113\n",
            "Epoch 6: val_accuracy did not improve from 0.27448\n",
            "135/135 [==============================] - 116s 857ms/step - loss: 4.4547 - accuracy: 0.3113 - val_loss: 5.5743 - val_accuracy: 0.2714\n",
            "Epoch 7/40\n",
            "135/135 [==============================] - ETA: 0s - loss: 4.1128 - accuracy: 0.3320\n",
            "Epoch 7: val_accuracy did not improve from 0.27448\n",
            "135/135 [==============================] - 116s 857ms/step - loss: 4.1128 - accuracy: 0.3320 - val_loss: 5.5663 - val_accuracy: 0.2604\n",
            "Epoch 8/40\n",
            "135/135 [==============================] - ETA: 0s - loss: 3.7931 - accuracy: 0.3564\n",
            "Epoch 8: val_accuracy did not improve from 0.27448\n",
            "135/135 [==============================] - 125s 927ms/step - loss: 3.7931 - accuracy: 0.3564 - val_loss: 5.7362 - val_accuracy: 0.2414\n",
            "Epoch 9/40\n",
            "135/135 [==============================] - ETA: 0s - loss: 3.4753 - accuracy: 0.3861\n",
            "Epoch 9: val_accuracy did not improve from 0.27448\n",
            "135/135 [==============================] - 116s 859ms/step - loss: 3.4753 - accuracy: 0.3861 - val_loss: 5.7525 - val_accuracy: 0.2497\n",
            "Epoch 9: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3c71630790>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "epochs = 40  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.compile(\n",
        "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds,callbacks=[checkpoint,early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ar_vocab = ar_vectorization.get_vocabulary()\n",
        "ar_index_lookup = dict(zip(range(len(ar_vocab)), ar_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence, reference_translation):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = ar_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = ar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    # Calculate the BLEU score for the decoded sentence\n",
        "    reference_translation = word_tokenize(reference_translation)\n",
        "    chencherry = SmoothingFunction()\n",
        "    decoded_sentence = word_tokenize(decoded_sentence[1:-1]) # remove start and end tokens\n",
        "    bleu_score = round(sentence_bleu([reference_translation], decoded_sentence,smoothing_function=chencherry.method1),4)\n",
        "    bleu_score1 = round(sentence_bleu([reference_translation], decoded_sentence,(1,0,0,0),smoothing_function=chencherry.method1),4)\n",
        "    return decoded_sentence, bleu_score , bleu_score1\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "total_bleu_score = 0.0\n",
        "total_bleu1_score = 0.0\n",
        "for i in range(50):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    reference_translation = test_pairs[test_eng_texts.index(input_sentence)][1]\n",
        "    decoded, bleu_score , bleu1_score = decode_sequence(input_sentence, reference_translation)\n",
        "    total_bleu_score += bleu_score\n",
        "    total_bleu1_score += bleu1_score\n",
        "    print(f\"Input sentence: {input_sentence}\")\n",
        "    print(f\"Reference translation: {reference_translation}\")\n",
        "    print(f\"Decoded sentence: {' '.join(decoded)}\\n\")\n",
        "    print(f\"BLEU score: {bleu_score}\")\n",
        "    print(f\"BLEU 1-gram score: {bleu1_score}\\n\")\n",
        "\n",
        "average_bleu_score = total_bleu_score / 50\n",
        "average_bleu1_score = total_bleu1_score / 50\n",
        "print(f\"Average BLEU score: {round(average_bleu_score,4)}\")\n",
        "print(f\"Average BLEU 1-gram score: {round(average_bleu1_score,4)}\")\n"
      ],
      "metadata": {
        "id": "8f10XZWNE7wO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb9b300-9296-4e89-cbba-7e7f83682c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence: join us for a gvmeetup in tunis on november global voices.\n",
            "Reference translation: [start] الاعلان عن ملتقى الاصوات العالمية في تونس يوم نوفمبر الاصوات العالمية [end]\n",
            "Decoded sentence: start ] الكويت يوم من سبتمبر ايلول في الاصوات العالمية [ end\n",
            "\n",
            "BLEU score: 0.1786\n",
            "BLEU 1-gram score: 0.4395\n",
            "\n",
            "Input sentence: Pass me the butter, please.\n",
            "Reference translation: [start] من فضلك ناولني الزبدة. [end]\n",
            "Decoded sentence: start ] [ UNK ] إلى هنا [ end\n",
            "\n",
            "BLEU score: 0.0636\n",
            "BLEU 1-gram score: 0.5338\n",
            "\n",
            "Input sentence: erkan s field diary discusses turkey s lifting of the headscarf hijab ban in his country.\n",
            "Reference translation: [start] مدونة حقول اركان انكليزي تناقش رفع الحظر عن الحجاب في تركيا [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] [ UNK ] في لبنان من الاردن رسالة الى الولايات المتحدة الامريكية [ UNK ] في مدينة [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0215\n",
            "BLEU 1-gram score: 0.2333\n",
            "\n",
            "Input sentence: The boy ran away.\n",
            "Reference translation: [start] هرب الولد. [end]\n",
            "Decoded sentence: start ] [ UNK ] الباب [ end\n",
            "\n",
            "BLEU score: 0.0811\n",
            "BLEU 1-gram score: 0.6619\n",
            "\n",
            "Input sentence: ellery with global voices friends in berlin july photo subhashish panigrahi.\n",
            "Reference translation: [start] اليري مع اصدقاء من الاصوات العالمية في برلين في يوليو تموز تصوير صبحشيش بانيجراهي [end]\n",
            "Decoded sentence: start ] جاينين مينديس فرانكو في يوم من اكتوبر تشرين الاول [ UNK ] [ UNK ] في يوم الاثنين الرابع عشر من اكتوبر تشرين الثان\n",
            "\n",
            "BLEU score: 0.0217\n",
            "BLEU 1-gram score: 0.3077\n",
            "\n",
            "Input sentence: a gang of young supporters of golden dawn the startlingly successful greek neo nazi political party attacked an approximately year old pakistani man on may in athens in the metro station of agios nikolaos a neighborhood known for its large immigrant population.\n",
            "Reference translation: [start] قامت مجموعة مكونة من الى شابا من مويدي حزب الفجر الذهبي معظم الروابط باليونانية الحزب اليوناني الذي حقق نجاحا مذهلا بالاعتداء علي رجل باكستاني الاصل يبلغ من العمر ما يقارب الثلاثين عاما في الخامس والعشرين من شهر مايو ايار في العاصمة اثينا بمحطة مترو منطقة اجيوس نيكولاوس وهو حي معروف بكبر عدد سكانه من المهاجرين [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] في مدينة [ UNK ] [ UNK ] [ UNK ] [ UNK ] [ UNK ] في ظل [ UNK ] من العمر عاما بينما كان هناك [ UNK ] بتبن\n",
            "\n",
            "BLEU score: 0.0103\n",
            "BLEU 1-gram score: 0.1479\n",
            "\n",
            "Input sentence: This desk takes up too much room.\n",
            "Reference translation: [start] هذا المكتب يأخذ مساحة كبيرة. [end]\n",
            "Decoded sentence: start ] [ UNK ] هذا الصباح من [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0591\n",
            "BLEU 1-gram score: 0.5385\n",
            "\n",
            "Input sentence: You'd better start now.\n",
            "Reference translation: [start] من الأفضل لك أن تبدأ الآن. [end]\n",
            "Decoded sentence: start ] [ UNK ] إلى أي شيء [ end\n",
            "\n",
            "BLEU score: 0.0517\n",
            "BLEU 1-gram score: 0.4445\n",
            "\n",
            "Input sentence: the daily star project redesign by beirut spring.\n",
            "Reference translation: [start] مشروع اعادة تصميم موقع صحيفة الديلي ستار من قبل ربيع بيروت [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] من اجل الحرية لايران والدول المجاورة لها صوت من العام الماضي [ end\n",
            "\n",
            "BLEU score: 0.0314\n",
            "BLEU 1-gram score: 0.3333\n",
            "\n",
            "Input sentence: Where's the money?\n",
            "Reference translation: [start] أين المال؟ [end]\n",
            "Decoded sentence: start ] أين أقرب صيدلية [ end\n",
            "\n",
            "BLEU score: 0.1782\n",
            "BLEU 1-gram score: 0.6192\n",
            "\n",
            "Input sentence: nearly of young iranians use illegal internet circumvention tools global voices.\n",
            "Reference translation: [start] يلجا من الشباب الايراني لطرق غير قانونية للاتصال بالانترنت الاصوات العالمية [end]\n",
            "Decoded sentence: start ] في [ UNK ] [ UNK ] [ UNK ] عن حرية التعبير وحقوق الانسان الاصوات العالمية [ end\n",
            "\n",
            "BLEU score: 0.1453\n",
            "BLEU 1-gram score: 0.381\n",
            "\n",
            "Input sentence: He closely resembles his father.\n",
            "Reference translation: [start] إنه يشبه أباه كثيراً. [end]\n",
            "Decoded sentence: start ] لقد [ UNK ] [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0624\n",
            "BLEU 1-gram score: 0.5455\n",
            "\n",
            "Input sentence: happiness value index for the african continent via afrigraphique cc nc.\n",
            "Reference translation: [start] موشر السعادة للقارة الافريقية عبر افروجرافيك استخدم تحت رخصة المشاع الابداعي [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] في مدينة [ UNK ] البرازيل من العمر [ end\n",
            "\n",
            "BLEU score: 0.0358\n",
            "BLEU 1-gram score: 0.3333\n",
            "\n",
            "Input sentence: saudi arabia.\n",
            "Reference translation: [start] السعودية [end]\n",
            "Decoded sentence: start ] السعودية [ end\n",
            "\n",
            "BLEU score: 0.6703\n",
            "BLEU 1-gram score: 0.6703\n",
            "\n",
            "Input sentence: Come on. Let's go home.\n",
            "Reference translation: [start] هيا. دعونا نذهب إلى البيت. [end]\n",
            "Decoded sentence: start ] تعال إلى هنا [ end\n",
            "\n",
            "BLEU score: 0.0443\n",
            "BLEU 1-gram score: 0.3031\n",
            "\n",
            "Input sentence: Can you guess what I have?\n",
            "Reference translation: [start] أيمكنك أن تحزر ما لديّ؟ [end]\n",
            "Decoded sentence: start ] هل لديك ما تقوله صحيح [ end\n",
            "\n",
            "BLEU score: 0.0607\n",
            "BLEU 1-gram score: 0.4449\n",
            "\n",
            "Input sentence: Where did you go?\n",
            "Reference translation: [start] أين ذهبت؟ [end]\n",
            "Decoded sentence: start ] أين سأذهب [ end\n",
            "\n",
            "BLEU score: 0.182\n",
            "BLEU 1-gram score: 0.5971\n",
            "\n",
            "Input sentence: you joined global voices in june what drew you to global voices.\n",
            "Reference translation: [start] لقد انضممت الى الاصوات العالمية في حزيران ما الذي كان يشدك الى الاصوات العالمية؟ [end]\n",
            "Decoded sentence: start ] قطر تحرش في الاصوات العالمية في اكتوبر تشرين الاول [ end\n",
            "\n",
            "BLEU score: 0.066\n",
            "BLEU 1-gram score: 0.3143\n",
            "\n",
            "Input sentence: How long do you play tennis every day?\n",
            "Reference translation: [start] كم ساعة في اليوم تلعب التنس؟ [end]\n",
            "Decoded sentence: start ] كم من الوقت هنا [ end\n",
            "\n",
            "BLEU score: 0.1048\n",
            "BLEU 1-gram score: 0.3791\n",
            "\n",
            "Input sentence: The meat tastes bad.\n",
            "Reference translation: [start] طعم اللحم سيء. [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0699\n",
            "BLEU 1-gram score: 0.6\n",
            "\n",
            "Input sentence: I need a knife.\n",
            "Reference translation: [start] أحتاج الى سكين. [end]\n",
            "Decoded sentence: start ] أحتاج [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.1464\n",
            "BLEU 1-gram score: 0.6815\n",
            "\n",
            "Input sentence: libya protests against gaddafi start ahead of schedule global voices.\n",
            "Reference translation: [start] ليبيا اندلاع المظاهرات ضد القذافي مبكرا الاصوات العالمية [end]\n",
            "Decoded sentence: start ] ليبيا [ UNK ] في [ UNK ] [ UNK ] [ UNK ] [ UNK ] الاصوات العالمية [ end\n",
            "\n",
            "BLEU score: 0.1588\n",
            "BLEU 1-gram score: 0.3913\n",
            "\n",
            "Input sentence: give books to help the education of syrian children through just giving website books syria.\n",
            "Reference translation: [start] تبرعوا بالكتب للمساعدة في تعليم اطفال سوريا من خلال موقع عطاء فقط كتب من اجل سوريا [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] [ UNK ] في امريكا اللاتينية [ UNK ] من اجل سوريا [ end\n",
            "\n",
            "BLEU score: 0.2033\n",
            "BLEU 1-gram score: 0.4545\n",
            "\n",
            "Input sentence: jordanian moey complains about the increase in alcohol prices in his country.\n",
            "Reference translation: [start] المدون الاردني يشتكي من ارتفاع اسعار الخمور في بلده [end]\n",
            "Decoded sentence: start ] المدون [ UNK ] انكليزي من مدونة تكتب في مدونة [ UNK ] عن الوضع في سوريا [ end\n",
            "\n",
            "BLEU score: 0.0658\n",
            "BLEU 1-gram score: 0.4286\n",
            "\n",
            "Input sentence: The lecture lasted for two hours.\n",
            "Reference translation: [start] استمرت المحاضرة مدة ساعتين. [end]\n",
            "Decoded sentence: start ] استمرت الحرب قرابة السنتين [ end\n",
            "\n",
            "BLEU score: 0.1188\n",
            "BLEU 1-gram score: 0.4296\n",
            "\n",
            "Input sentence: san jose u s a mary aviles.\n",
            "Reference translation: [start] سان خوسيه الولايات المتحدة الامريكية ماري افيليس [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] [ UNK ] في ظل المناخ [ end\n",
            "\n",
            "BLEU score: 0.0407\n",
            "BLEU 1-gram score: 0.375\n",
            "\n",
            "Input sentence: the status quo continuing the present situation and making iran a second pakistan or renewing the islamic republic there is no third option the choice is either to accept institutionalised corruption or fight against it to realise the goals of late ayathollah khomeini the leader of the islamic revolution.\n",
            "Reference translation: [start] الوضع الراهن الاستمرار في هذا الوضع الحالي وتحويل ايران الى باكستان ثانية او اعادة تجديد الجمهورية الاسلامية ليس هناك خيار ثالث الخيار هو بين قبول الفساد الموسسي او مكافحته لتحقيق اهداف اية الله الخميني قايد الثورة الاسلامية [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] في الصين الجنوبي ان يكون ممتعا [ UNK ] [ UNK ] من خلال فترة قصيرة من قبل السلطات الحاكمة الغير قانوني\n",
            "\n",
            "BLEU score: 0.0113\n",
            "BLEU 1-gram score: 0.1297\n",
            "\n",
            "Input sentence: I'll take this one.\n",
            "Reference translation: [start] سآخذ هذا. [end]\n",
            "Decoded sentence: start ] [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0822\n",
            "BLEU 1-gram score: 0.6441\n",
            "\n",
            "Input sentence: an urban dwelling made from discarded materials and found furniture photo by alejandro rustom copyright demotix.\n",
            "Reference translation: [start] استعدادات للسكن معدة من المواد والاثاث المهمل الصورة بواسطة اليخاندرو رستم حقوق الملكية لموقع ديموتكس [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] [ UNK ] في مدونته من قبل الكاتب [ UNK ] الصورة من موقع فليكر استخدمت تحت رخصة المشاع الابداعي [ end\n",
            "\n",
            "BLEU score: 0.0222\n",
            "BLEU 1-gram score: 0.2667\n",
            "\n",
            "Input sentence: jordan trusting arab online services global voices.\n",
            "Reference translation: [start] الاردن الثقة بخدمات الانترنت العربية الاصوات العالمية [end]\n",
            "Decoded sentence: start ] الاردن رسالة الى الانترنت من اجل حقوق الانسان الاصوات العالمية [ end\n",
            "\n",
            "BLEU score: 0.2658\n",
            "BLEU 1-gram score: 0.5714\n",
            "\n",
            "Input sentence: in fact saudi arabia s discriminatory male guardianship system is still in place under this system women require the consent or presence of a male relative guardian to travel outside the country apply for passport get married or to even rent their own places many human rights defenders and activists including essam koshak and mariam al otaibi have been arrested for participating in the campaign iammyownguardian which calls for an end to the male guardianship system.\n",
            "Reference translation: [start] كما ان نظام الولاية على المراة ما يزال نافذ المفعول في الممكلة حيث ينص على ان تظهر المراة بوجود ولي الامر الذكر او موافقته للسفر خارج البلاد او تقديم طلب للحصول على جواز سفر او الزواج او حتى توقيع عقد اجار وقد تم اعتقال العديد من المدافعين والمدافعات عن حقوق الانسان والناشطين بما في ذلك عصام كوشك ومريم العتيبي بسبب مشاركتهم في حملة انا وصية نفسي المطالبة بانهاء نظام الولاية [end]\n",
            "Decoded sentence: start ] في الحقيقة اود ان استذكر من خلال فترة [ UNK ] من خلال فترة [ UNK ] [ UNK ] من ضمنها عملية لافا جاتو ا\n",
            "\n",
            "BLEU score: 0.0036\n",
            "BLEU 1-gram score: 0.0515\n",
            "\n",
            "Input sentence: Which one do you prefer, the red one or the blue one?\n",
            "Reference translation: [start] أيهما تفضل الأحمر أم الازرق ؟ [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] [ UNK ] إلى المستشفى [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0358\n",
            "BLEU 1-gram score: 0.3333\n",
            "\n",
            "Input sentence: kurdish media outlets are abuzz with a photograph of a peshmerga woman sitting beside an automatic weapon breastfeeding her child the picture has been widely distributed on social networking sites highlighting the strength of kurdish women and the resilience of female combatants in the ongoing fight against isis.\n",
            "Reference translation: [start] اثارت وسايل الاعلام الكردية صورة لمقاتلة من البشمركة وهي جالسة بجانب سلاح رشاش اثناء قيامها بارضاع طفل انتشرت الصورة على نطاق واسع على وسايل الاعلام الاجتماعي في سياق ابراز قوة الكرديات وصمود المقاتلات في القتال الداير ضد داعش [end]\n",
            "Decoded sentence: start ] [ UNK ] الشرطة المصرية [ UNK ] [ UNK ] [ UNK ] في [ UNK ] [ UNK ] [ UNK ] [ UNK ] [ UNK ] [ UNK ] صداقة شخصية صينية في [ UNK ] [ UNK ] المو\n",
            "\n",
            "BLEU score: 0.0116\n",
            "BLEU 1-gram score: 0.1522\n",
            "\n",
            "Input sentence: I'll fix you some coffee.\n",
            "Reference translation: [start] سأعد لك بعض القهوة. [end]\n",
            "Decoded sentence: start ] أنا رجل لطيف [ end\n",
            "\n",
            "BLEU score: 0.0558\n",
            "BLEU 1-gram score: 0.3227\n",
            "\n",
            "Input sentence: welovehani it took only a day to arrest the pastor who was convicted of spreading false rumors about park geun hye however with cho hyun oh who was convicted of spreading false rumors about roh it took a year and nine months to finally bring him to trial.\n",
            "Reference translation: [start] لم ياخذ اكثر من يوم القاء القبض على القس المتهم بنشر اشاعات كاذبة عن باك كون هيه في حين اخذ الامر سنة وتسعة شهور مع تشو هيون اوه المتهم بنشر اشاعات كاذبة عن روه [end]\n",
            "Decoded sentence: start ] بعد ان كان [ UNK ] [ UNK ] في يوم من الأيام كان قد [ UNK ] [ UNK ] [ UNK ] [ UNK ] من مارس اذار يوما ف\n",
            "\n",
            "BLEU score: 0.0137\n",
            "BLEU 1-gram score: 0.1972\n",
            "\n",
            "Input sentence: Are these books yours?\n",
            "Reference translation: [start] هل هي كتبك ؟ [end]\n",
            "Decoded sentence: start ] هل هذا الكتاب [ end\n",
            "\n",
            "BLEU score: 0.1339\n",
            "BLEU 1-gram score: 0.4653\n",
            "\n",
            "Input sentence: Take it easy.\n",
            "Reference translation: [start] لا تحمل هماً. [end]\n",
            "Decoded sentence: start ] [ UNK ] من الكتاب [ end\n",
            "\n",
            "BLEU score: 0.071\n",
            "BLEU 1-gram score: 0.5966\n",
            "\n",
            "Input sentence: What does your aunt do?\n",
            "Reference translation: [start] ماذا تعمل عمتك؟ [end]\n",
            "Decoded sentence: start ] ماذا تعتقد أفضل [ end\n",
            "\n",
            "BLEU score: 0.1545\n",
            "BLEU 1-gram score: 0.5368\n",
            "\n",
            "Input sentence: Do not open your book.\n",
            "Reference translation: [start] لا تفتح كتابك. [end]\n",
            "Decoded sentence: start ] هل لديك موعد [ end\n",
            "\n",
            "BLEU score: 0.0644\n",
            "BLEU 1-gram score: 0.3723\n",
            "\n",
            "Input sentence: My dream is to become a firefighter.\n",
            "Reference translation: [start] أريد أن أكون رجل إطفاء. [end]\n",
            "Decoded sentence: start ] أخي صغير هو [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0572\n",
            "BLEU 1-gram score: 0.4912\n",
            "\n",
            "Input sentence: We are going to travel abroad this summer.\n",
            "Reference translation: [start] سنسافر إلى الخارج هذا الصيف. [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] في اليابان اليوم [ end\n",
            "\n",
            "BLEU score: 0.0514\n",
            "BLEU 1-gram score: 0.4615\n",
            "\n",
            "Input sentence: image from flickr user pablodf pablo flores cc by nc nd.\n",
            "Reference translation: [start] صورة من فليكر للمستخدم [end]\n",
            "Decoded sentence: start ] الصورة من موقع اوستاغرام الروسي [ UNK ] [ UNK ] [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0372\n",
            "BLEU 1-gram score: 0.3889\n",
            "\n",
            "Input sentence: why do you travel share your stories reasons or thoughts with the hashtag yoviajopara.\n",
            "Reference translation: [start] لماذا تسافر؟ شارك بقصصك باسبابك او افكارك فى هاشتاج [end]\n",
            "Decoded sentence: start ] لماذا يخاف الامريكيون من المهرجين واكره المستشفيات المدنية [ end\n",
            "\n",
            "BLEU score: 0.0826\n",
            "BLEU 1-gram score: 0.3245\n",
            "\n",
            "Input sentence: according to the bribr manifesto ru.\n",
            "Reference translation: [start] وفقا للبيان التاسيسي لبريبر جميع الروابط بالروسية والانجليزية [end]\n",
            "Decoded sentence: start ] وفقا [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0888\n",
            "BLEU 1-gram score: 0.4133\n",
            "\n",
            "Input sentence: sumito estevez sumitoestevez august.\n",
            "Reference translation: [start] ما اجمل هذا الوسم الذي اطلقه المكسيكيون تضامن يظهر الجوانب المعنوية فتلك هي الروح الطيبة [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] [ UNK ] في ظل المناخ [ end\n",
            "\n",
            "BLEU score: 0.0298\n",
            "BLEU 1-gram score: 0.2744\n",
            "\n",
            "Input sentence: some uber drivers trick their customers alexander warns they arrive turn on their meter and only then call the client but the rules say they re supposed to wait to activate the meter until after they call the client.\n",
            "Reference translation: [start] بعض سايقي اوبر يحتالون على الزباين الكسندر٢ محذرا فعندما يصلون الى المكان المحدد يشغلون العداد ومن ثم يتصلون بالزباين بينما تنص التعليمات بوضوح على ان تشغيل العداد لا يبدا الا بعد اتصالهم بالزباين [end]\n",
            "Decoded sentence: start ] [ UNK ] [ UNK ] [ UNK ] في مدونته عن [ UNK ] [ UNK ] [ UNK ] [ UNK ] الى زيادة ملحوظة في مدينة [ UNK ] [ UNK ] [ UNK ] [ UNK ] بتبن\n",
            "\n",
            "BLEU score: 0.0116\n",
            "BLEU 1-gram score: 0.1364\n",
            "\n",
            "Input sentence: He lives in Tokyo.\n",
            "Reference translation: [start] إنه يسكن في طوكيو. [end]\n",
            "Decoded sentence: start ] إنه في طوكيو [ end\n",
            "\n",
            "BLEU score: 0.1357\n",
            "BLEU 1-gram score: 0.5647\n",
            "\n",
            "Input sentence: Do you play soccer?\n",
            "Reference translation: [start] هل تعرف كيف تلعب كرة القدم؟ [end]\n",
            "Decoded sentence: start ] هل تحب كرة القدم [ end\n",
            "\n",
            "BLEU score: 0.1097\n",
            "BLEU 1-gram score: 0.4549\n",
            "\n",
            "Input sentence: For me, it's important.\n",
            "Reference translation: [start] بالنسبة لي، إنه مهم. [end]\n",
            "Decoded sentence: start ] [ UNK ] ذلك الوقت [ end\n",
            "\n",
            "BLEU score: 0.0636\n",
            "BLEU 1-gram score: 0.5338\n",
            "\n",
            "Input sentence: There is a basket under the table.\n",
            "Reference translation: [start] توجد سلة تحت الطاولة. [end]\n",
            "Decoded sentence: start ] هناك [ UNK ] [ UNK ] في [ UNK ] [ end\n",
            "\n",
            "BLEU score: 0.0437\n",
            "BLEU 1-gram score: 0.4\n",
            "\n",
            "Average BLEU score: 0.0902\n",
            "Average BLEU 1-gram score: 0.4134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0K7B8n-94fr9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}